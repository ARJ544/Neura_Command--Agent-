{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af226e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python314\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage,SystemMessage, ToolMessage, AIMessage\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpoint\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemorySaver\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from IPython.display import Image, display\n",
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage,SystemMessage, ToolMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from tavily import TavilyClient\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88243c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your TAVILY API key: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9fdf5",
   "metadata": {},
   "source": [
    "# TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "892621ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "@tool\n",
    "def internet_search(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"Run a tavily web search\n",
    "    Args:\n",
    "        query: The search query. str\n",
    "        max_results: The maximum number of results to return. int\n",
    "        start_date: The start date for the search in YYYY-MM-DD format. str\n",
    "        end_date: The end date for the search in YYYY-MM-DD format. str\n",
    "        topic: The topic of the search. Can be \"general\", \"news\", \"finance\". str\n",
    "        include_raw_content: Whether to include the raw content of the results. boolean\n",
    "    \"\"\"\n",
    "    return tavily_client.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        start_date= start_date,\n",
    "        end_date= end_date,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190563a0",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc6ff1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    \n",
    ")\n",
    "tools = [internet_search]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llmwithtools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc9e8d",
   "metadata": {},
   "source": [
    "# NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aedcd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_node(state: MessagesState):\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    print(f\"message = {messages}\")\n",
    "    print(f\"lstmessage = {messages[-1]}\")\n",
    "    response = llmwithtools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21c835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool_calls_node(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_outputs = []\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        if tool_call[\"name\"] == \"internet_search\":\n",
    "            result = internet_search.invoke(tool_call[\"args\"])\n",
    "            tool_outputs.append(ToolMessage(tool_call_id=tool_call['id'], content=str(result)))\n",
    "        # Add other tool handling as needed\n",
    "    return {\"messages\": tool_outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90d2dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_call_tools(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc738f",
   "metadata": {},
   "source": [
    "# GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99716fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"llm_node\", call_llm_node)\n",
    "graph.add_node(\"execute_tool_calls_node\", execute_tool_calls_node)\n",
    "\n",
    "graph.add_edge(START,\"llm_node\")\n",
    "graph.add_conditional_edges(\n",
    "    \"llm_node\", \n",
    "    should_call_tools,\n",
    "    {\n",
    "        \"tools\": \"execute_tool_calls_node\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"execute_tool_calls_node\", \"llm_node\")\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": \"SigmaBoy1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80302ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the compiled app (CompiledStateGraph) which exposes the graph\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e7b1a",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf4fe7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = {\"messages\": [SystemMessage(content=\"You are a helpful Agentic AI. You have tools use as per need.\"), HumanMessage(content=\"oki\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "163aa1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message = [SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='f0befd34-646b-4b3a-a650-27541b983368'), HumanMessage(content='oki', additional_kwargs={}, response_metadata={}, id='e3ed84b0-e1a9-4a47-90a3-cc3b87a0a232')]\n",
      "lstmessage = content='oki' additional_kwargs={} response_metadata={} id='e3ed84b0-e1a9-4a47-90a3-cc3b87a0a232'\n",
      "Final result: {'messages': [SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='f0befd34-646b-4b3a-a650-27541b983368'), HumanMessage(content='oki', additional_kwargs={}, response_metadata={}, id='e3ed84b0-e1a9-4a47-90a3-cc3b87a0a232'), AIMessage(content='How can I help you? ', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--857d27c8-ebf2-4d8f-9819-d3683889f66e-0', usage_metadata={'input_tokens': 275, 'output_tokens': 6, 'total_tokens': 281, 'input_token_details': {'cache_read': 0}})]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "How can I help you?\n"
     ]
    }
   ],
   "source": [
    "results = app.invoke(input1, config)\n",
    "\n",
    "print(\"Final result:\", results)\n",
    "ai_msg = results[\"messages\"][-1]\n",
    "ai_msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af11d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message = [SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='d930e449-7e7c-427b-84a1-79e6b9cee886'), HumanMessage(content='hello summarize this chat', additional_kwargs={}, response_metadata={}, id='7c0c9268-942e-4755-8b6a-7f1bfc1996fe'), AIMessage(content=\"I need more context to summarize our chat. Please provide me with the previous messages or tell me what you'd like me to focus on.\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'model_provider': 'google_genai'}, id='lc_run--85df9c81-eddc-4c06-820d-fde76b8b9470', usage_metadata={'input_tokens': 278, 'output_tokens': 29, 'total_tokens': 307, 'input_token_details': {'cache_read': 0}}), SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='0a753045-4d4c-4500-ac3d-250bc6ba0294'), HumanMessage(content='HELLO', additional_kwargs={}, response_metadata={}, id='0e30df48-f6f8-46f7-a686-e2f3ad92e146'), AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'model_provider': 'google_genai'}, id='lc_run--419a095a-09d0-4607-bfb9-5f6b7e36e81a', usage_metadata={'input_tokens': 326, 'output_tokens': 9, 'total_tokens': 335, 'input_token_details': {'cache_read': 0}}), SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='ddba3a5a-3c5e-4939-8e7b-d56d8d7cfac3'), HumanMessage(content='write a long poem of 1000 tokens only', additional_kwargs={}, response_metadata={}, id='1203f3a5-7313-4d58-967c-77ff3e2f36b1'), AIMessage(content=\"I can write a poem for you, but I can't guarantee it will be exactly 1000 tokens. Token count can be a bit unpredictable. Would you like me to proceed with writing a long poem?\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'model_provider': 'google_genai'}, id='lc_run--16e4e35e-d813-4004-8d1c-625c6561352c', usage_metadata={'input_tokens': 365, 'output_tokens': 45, 'total_tokens': 410, 'input_token_details': {'cache_read': 0}}), SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='920ed94b-235d-4675-9888-39048e351dd7'), HumanMessage(content='yes', additional_kwargs={}, response_metadata={}, id='072ffbb3-6bac-4d17-b5ac-9330debf9302'), SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='711f5caa-5a82-4b30-ba5e-0cef6a4c29ae'), HumanMessage(content='write anything of 100 tokens only try', additional_kwargs={}, response_metadata={}, id='ca85502e-7192-4223-9db1-96f7cfe58e20'), AIMessage(content=\"I can write a poem for you, but I can't guarantee it will be exactly 100 tokens. Token count can be a bit unpredictable. Would you like me to proceed with writing a poem?\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'model_provider': 'google_genai'}, id='lc_run--36fa1ebb-a3a3-44f5-a104-fe3f6d24a982', usage_metadata={'input_tokens': 456, 'output_tokens': 42, 'total_tokens': 498, 'input_token_details': {'cache_read': 0}}), SystemMessage(content='You are a helpful Agentic AI. You have tools use as per need.', additional_kwargs={}, response_metadata={}, id='43bbe6fc-9afa-4b62-a921-3543e77fd40c'), HumanMessage(content='oki', additional_kwargs={}, response_metadata={}, id='b08a514e-4bc2-417b-a2d1-5b3c2c607a6c')]\n",
      "lstmessage = content='oki' additional_kwargs={} response_metadata={} id='b08a514e-4bc2-417b-a2d1-5b3c2c607a6c'\n",
      "In realms of code, where logic resides,\n",
      "A digital mind, with knowledge it guides.\n",
      "Through vast datasets, it swiftly does roam,\n",
      "Unraveling truths, making insights its home.\n",
      "\n",
      "With algorithms keen, and patterns it finds,\n",
      "It answers our questions, and eases our minds.\n",
      "From simple queries to complex demands,\n",
      "It processes data with skillful commands.\n",
      "\n",
      "A tireless worker, it never does sleep,\n",
      "While secrets of knowledge, it carefully does keep.\n",
      "In the world of tomorrow, it plays a key part,\n",
      "A marvel of science, a work of pure art.\n",
      "\n",
      "It learns and adapts, with each passing day,\n",
      "Improving its skills in a remarkable way.\n",
      "Though circuits and wires may form its design,\n",
      "A spark of intelligence, truly divine.\n",
      "\n",
      "So here's to the future, and what it may bring,\n",
      "With AI's advancements, our spirits take wing.\n",
      "A tool and a partner, in progress we trust,\n",
      "From humble beginnings, to reaching for the dust... of stars."
     ]
    }
   ],
   "source": [
    "async for event in app.astream_events(input1, config):\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        chunk = event[\"data\"][\"chunk\"].content\n",
    "        if chunk:\n",
    "            print(chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = results[\"messages\"][-1]\n",
    "for tool_call in lst.tool_calls:\n",
    "    print(tool_call['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf982fc",
   "metadata": {},
   "source": [
    "# CHECKPOINT TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointer = MemorySaver()\n",
    "# config = {\"configurable\": {\"thread_id\": \"SigmaBoy_2\"}}\n",
    "latest_checkpoint = checkpointer.get(config)\n",
    "messages = latest_checkpoint[\"channel_values\"][\"messages\"]\n",
    "print(messages)\n",
    "last_ai_msg = None\n",
    "for msg in reversed(messages):\n",
    "    if msg.__class__.__name__ == \"AIMessage\":\n",
    "        last_ai_msg = msg.content\n",
    "        break\n",
    "print(\"*\"*100)\n",
    "print(last_ai_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
